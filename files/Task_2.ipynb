{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b5b6765-4338-4350-8f10-42834809124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "from pyspark.sql.functions import col, to_date, date_format, regexp_replace, when, concat_ws, coalesce, lit, udf, avg, first, last_day, round\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"setup\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50b14530-3b84-44b2-9703-04238ab56324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+------+------------+-------------------+--------------------+--------+-----+----+\n",
      "|order_id|delivery_company|quantity| price|ordered_date|            address|                 _c6|     _c7|  _c8| _c9|\n",
      "+--------+----------------+--------+------+------------+-------------------+--------------------+--------+-----+----+\n",
      "|       1| delivery_comp_1|       1|245,52|    9/2/2022| Cedar Lane Houston|            CA 90001|    null| null|null|\n",
      "|       2| delivery_comp_2|       2|114,77|        null|        Main Street|   New York CA 60601|    null| null|null|\n",
      "|       3| delivery_comp_3|    null|739,43|   14-3-2022|        Main Street|    Chicago TX 10001|    null| null|null|\n",
      "|       4| delivery_comp_0|       1|878.93|   20/4/2022|         Oak Avenue|Los Angeles FL 90001|    null| null|null|\n",
      "|       5| delivery_comp_1|       2|481,44|        null|Maple Drive Chicago|            FL 60601|    null| null|null|\n",
      "|       6| delivery_comp_2|     #NA| 78,13|        null|        Main Street|    Houston NY 77001|    null| null|null|\n",
      "|       7| delivery_comp_3|       1|832.17|   20-2-2022|Oak Avenue New York|            CA 10001|    null| null|null|\n",
      "|       8| delivery_comp_0|       2| 687,8|    1/4/2022|        Maple Drive|         Los Angeles|      CA|10001|null|\n",
      "|       9| delivery_comp_1|     #NA|338,44|   13/4/2022|   Cedar Lane Miami|            NY 77001|    null| null|null|\n",
      "|      10| delivery_comp_2|       1|461.33|        null|         Oak Avenue|             Chicago|NY 77001| null|null|\n",
      "|      11| delivery_comp_3|       2|544.33|    8/4/2022|         Oak Avenue|         Los Angeles|CA 90001| null|null|\n",
      "|      12| delivery_comp_0|      NA|200,20|        null|         Cedar Lane|Los Angeles FL 77001|    null| null|null|\n",
      "|      13| delivery_comp_1|       1|939,99|        null|        Main Street|             Chicago|      IL|33101|null|\n",
      "|      14| delivery_comp_2|       2| 72,69|    2/3/2022|         Oak Avenue|         Los Angeles|      NY|33101|null|\n",
      "|      15| delivery_comp_3|    null|635.28|   15-2-2022|        Main Street|               Miami|FL 10001| null|null|\n",
      "|      16| delivery_comp_0|       1|592.83|    2/3/2022|        Main Street|         Los Angeles|      FL|90001|null|\n",
      "|      17| delivery_comp_1|       2| 533.6|    2/3/2022|        Maple Drive|             Houston|TX 90001| null|null|\n",
      "|      18| delivery_comp_2|     NaN| 424.7|        null|         Cedar Lane|            New York|TX 33101| null|null|\n",
      "|      19| delivery_comp_3|       1|889,97|    4/3/2022|         Elm Street|      Miami TX 90001|    null| null|null|\n",
      "|      20| delivery_comp_0|       2|222.12|   20/3/2022|        Maple Drive|               Miami|IL 77001| null|null|\n",
      "+--------+----------------+--------+------+------------+-------------------+--------------------+--------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_csv(input_path):\n",
    "\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = spark.read.option(\"delimiter\", \",\").option(\"quote\", \"\\\"\").csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "orders = read_csv('cleaned.csv')\n",
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b6e7545-d223-43c1-b716-ed7a61dd43b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+------+------------+--------------------+\n",
      "|order_id|delivery_company|quantity| price|ordered_date|             address|\n",
      "+--------+----------------+--------+------+------------+--------------------+\n",
      "|       1| delivery_comp_1|       1|245.52|  09-02-2022|Cedar Lane,Housto...|\n",
      "|       2| delivery_comp_2|       2|114.77|        null|Main Street,New,Y...|\n",
      "|       3| delivery_comp_3|    null|739.43|  14-03-2022|Main Street,Chica...|\n",
      "|       4| delivery_comp_0|       1|878.93|  20-04-2022|Oak Avenue,Los,An...|\n",
      "|       5| delivery_comp_1|       2|481.44|        null|Maple Drive,Chica...|\n",
      "|       6| delivery_comp_2|     #NA| 78.13|        null|Main Street,Houst...|\n",
      "|       7| delivery_comp_3|       1|832.17|  20-02-2022|Oak Avenue,New,Yo...|\n",
      "|       8| delivery_comp_0|       2| 687.8|  01-04-2022|Maple Drive,Los,A...|\n",
      "|       9| delivery_comp_1|     #NA|338.44|  13-04-2022|Cedar Lane,Miami,...|\n",
      "|      10| delivery_comp_2|       1|461.33|        null|Oak Avenue,Chicag...|\n",
      "|      11| delivery_comp_3|       2|544.33|  08-04-2022|Oak Avenue,Los,An...|\n",
      "|      12| delivery_comp_0|      NA| 200.2|        null|Cedar Lane,Los,An...|\n",
      "|      13| delivery_comp_1|       1|939.99|        null|Main Street,Chica...|\n",
      "|      14| delivery_comp_2|       2| 72.69|  02-03-2022|Oak Avenue,Los,An...|\n",
      "|      15| delivery_comp_3|    null|635.28|  15-02-2022|Main Street,Miami...|\n",
      "|      16| delivery_comp_0|       1|592.83|  02-03-2022|Main Street,Los,A...|\n",
      "|      17| delivery_comp_1|       2| 533.6|  02-03-2022|Maple Drive,Houst...|\n",
      "|      18| delivery_comp_2|     NaN| 424.7|        null|Cedar Lane,New,Yo...|\n",
      "|      19| delivery_comp_3|       1|889.97|  04-03-2022|Elm Street,Miami,...|\n",
      "|      20| delivery_comp_0|       2|222.12|  20-03-2022|Maple Drive,Miami...|\n",
      "+--------+----------------+--------+------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def process_df(df):\n",
    "    \n",
    "    def enforce_double_type(df, column_name):\n",
    "        # Check datatype of the column\n",
    "        column_type = [f.dataType for f in df.schema.fields if f.name == column_name][0]\n",
    "    \n",
    "        # If not double, cast it\n",
    "        \n",
    "        if not isinstance(column_type, DoubleType):\n",
    "    \n",
    "            # Replace commas with periods\n",
    "            df = df.withColumn(column_name, regexp_replace(col(column_name), \",\", \".\"))\n",
    "            \n",
    "            # Cast to DoubleType\n",
    "            df = df.withColumn(column_name, col(column_name).cast(DoubleType()))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    double_df = enforce_double_type(df, 'price')\n",
    "\n",
    "    merged_df = double_df.withColumn(\n",
    "    \"address_merged\", \n",
    "    concat_ws(\" \", \n",
    "              coalesce(double_df.address, lit(\"\")), \n",
    "              coalesce(double_df._c6, lit(\"\")),\n",
    "              coalesce(double_df._c7, lit(\"\")),\n",
    "              coalesce(double_df._c8, lit(\"\")), \n",
    "              coalesce(double_df._c9, lit(\"\")))\n",
    "    ).select('order_id', 'delivery_company', 'quantity', 'price', 'ordered_date', 'address_merged')\n",
    "    \n",
    "    # Python lambda to process the address\n",
    "    address_lambda = lambda x: str(x.split(' ')[0]) + ' ' + str(x.split(' ')[1]) + ',' + ','.join(x.split(' ')[2:])\n",
    "    \n",
    "    # Convert the lambda to a UDF\n",
    "    address_udf = udf(address_lambda, StringType())\n",
    "\n",
    "    # Apply the UDF to the DataFrame column\n",
    "    address_df = merged_df.withColumn(\"address\", address_udf(merged_df[\"address_merged\"]))\n",
    "\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "    def standardize_date_formats(df, column_name):\n",
    "        temp_df = df.withColumn(\n",
    "            column_name,\n",
    "            when(to_date(col(column_name), \"dd-MM-yyyy\").isNotNull(), to_date(col(column_name), \"dd-MM-yyyy\"))\n",
    "            .when(to_date(col(column_name), \"dd/MM/yyyy\").isNotNull(), to_date(col(column_name), \"dd/MM/yyyy\"))\n",
    "            .otherwise(col(column_name))  # Keeping original values if no format matches. Replace with .otherwise(None) to set unmatched to null\n",
    "        )\n",
    "        return temp_df.withColumn(\"ordered_date\", date_format(temp_df[\"ordered_date\"], \"dd-MM-yyyy\"))\n",
    "    \n",
    "    orders_new = standardize_date_formats(address_df, 'ordered_date')\n",
    "    final_orders = orders_new.drop(orders_new.address_merged)\n",
    "    return final_orders\n",
    "    \n",
    "orders = process_df(orders)\n",
    "orders.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
