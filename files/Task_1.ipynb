{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7711514c-e3d4-45b5-8b82-b3d4bcf944ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"setup\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ade5dc84-2d6e-44b3-bd35-79dc03e83db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    # Define a function to check if value is a date in format DD/MM/YYYY\n",
    "    def is_date(s):\n",
    "        pattern = '^\\d{1,2}(\\/|\\-)\\d{1,2}(\\/|\\-)\\d{4}$'\n",
    "        return bool(re.match(pattern, s))\n",
    "    \n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w', newline='') as outfile:\n",
    "        reader = csv.reader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        for i, row in enumerate(reader):\n",
    "            # Check the fifth value of the row, if exists\n",
    "            if not is_date(row[4]) and i > 0 and str(row[4]).lower() != 'null':\n",
    "                row[3] = str('') + str(row[3]) + ',' + str(row[4]) + str('')\n",
    "                row.pop(4)\n",
    "                writer.writerow(row)\n",
    "            else:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Example usage:\n",
    "process_csv('/home/jovyan/clean_me.csv', 'cleaned.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "590a4890-66b3-4196-b02d-221669192884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+------+------------+-------------------+--------------------+--------+-----+----+\n",
      "|order_id|delivery_company|quantity| price|ordered_date|            address|                 _c6|     _c7|  _c8| _c9|\n",
      "+--------+----------------+--------+------+------------+-------------------+--------------------+--------+-----+----+\n",
      "|       1| delivery_comp_1|       1|245,52|    9/2/2022| Cedar Lane Houston|            CA 90001|    null| null|null|\n",
      "|       2| delivery_comp_2|       2|114,77|        null|        Main Street|   New York CA 60601|    null| null|null|\n",
      "|       3| delivery_comp_3|    null|739,43|   14-3-2022|        Main Street|    Chicago TX 10001|    null| null|null|\n",
      "|       4| delivery_comp_0|       1|878.93|   20/4/2022|         Oak Avenue|Los Angeles FL 90001|    null| null|null|\n",
      "|       5| delivery_comp_1|       2|481,44|        null|Maple Drive Chicago|            FL 60601|    null| null|null|\n",
      "|       6| delivery_comp_2|     #NA| 78,13|        null|        Main Street|    Houston NY 77001|    null| null|null|\n",
      "|       7| delivery_comp_3|       1|832.17|   20-2-2022|Oak Avenue New York|            CA 10001|    null| null|null|\n",
      "|       8| delivery_comp_0|       2| 687,8|    1/4/2022|        Maple Drive|         Los Angeles|      CA|10001|null|\n",
      "|       9| delivery_comp_1|     #NA|338,44|   13/4/2022|   Cedar Lane Miami|            NY 77001|    null| null|null|\n",
      "|      10| delivery_comp_2|       1|461.33|        null|         Oak Avenue|             Chicago|NY 77001| null|null|\n",
      "|      11| delivery_comp_3|       2|544.33|    8/4/2022|         Oak Avenue|         Los Angeles|CA 90001| null|null|\n",
      "|      12| delivery_comp_0|      NA|200,20|        null|         Cedar Lane|Los Angeles FL 77001|    null| null|null|\n",
      "|      13| delivery_comp_1|       1|939,99|        null|        Main Street|             Chicago|      IL|33101|null|\n",
      "|      14| delivery_comp_2|       2| 72,69|    2/3/2022|         Oak Avenue|         Los Angeles|      NY|33101|null|\n",
      "|      15| delivery_comp_3|    null|635.28|   15-2-2022|        Main Street|               Miami|FL 10001| null|null|\n",
      "|      16| delivery_comp_0|       1|592.83|    2/3/2022|        Main Street|         Los Angeles|      FL|90001|null|\n",
      "|      17| delivery_comp_1|       2| 533.6|    2/3/2022|        Maple Drive|             Houston|TX 90001| null|null|\n",
      "|      18| delivery_comp_2|     NaN| 424.7|        null|         Cedar Lane|            New York|TX 33101| null|null|\n",
      "|      19| delivery_comp_3|       1|889,97|    4/3/2022|         Elm Street|      Miami TX 90001|    null| null|null|\n",
      "|      20| delivery_comp_0|       2|222.12|   20/3/2022|        Maple Drive|               Miami|IL 77001| null|null|\n",
      "+--------+----------------+--------+------+------------+-------------------+--------------------+--------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_csv(input_path):\n",
    "\n",
    "    # Read the CSV into a DataFrame\n",
    "    df = spark.read.option(\"delimiter\", \",\").option(\"quote\", \"\\\"\").csv(input_path, header=True, inferSchema=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "df = read_csv('cleaned.csv')\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
